{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install opencv-python\n",
    "# %pip install numpy==1.23.0\n",
    "# %pip install matplotlib\n",
    "# %pip install pandas\n",
    "# %pip install matrepr\n",
    "# %pip install mediapipe opencv-python-headless\n",
    "# !wget -q https://storage.googleapis.com/mediapipe-models/gesture_recognizer/gesture_recognizer/float16/1/gesture_recognizer.task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "from mediapipe.tasks import python\n",
    "from mediapipe.tasks.python import vision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Globals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_video = \"new_video.mov\" # Replace with path of input video\n",
    "output_video = \"output_video_clean\" \n",
    "output_directory = \"videoframes\"\n",
    "visualize_tracking = True # Set whether or not you want tracking info in the output video\n",
    "base_options = python.BaseOptions(model_asset_path='gesture_recognizer.task')\n",
    "options = vision.GestureRecognizerOptions(base_options=base_options)\n",
    "recognizer = vision.GestureRecognizer.create_from_options(options)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_video(frames, output_name):\n",
    "    height, width, _ = frames[0].shape\n",
    "    output_video_path = \"{}.mp4\".format(output_name)\n",
    "\n",
    "    # Define the codec and create VideoWriter object\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    video = cv2.VideoWriter(output_video_path, fourcc, 50, (width, height)) \n",
    "\n",
    "    for frame in frames:\n",
    "        video.write(frame)\n",
    "    \n",
    "    video.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split video into frames in the videoframes directory\n",
    "# Runtime: ~60 seconds\n",
    "\n",
    "# Delete directory if it already exists and make a new one\n",
    "if os.path.exists(output_directory):\n",
    "   shutil.rmtree(output_directory)\n",
    "os.makedirs(output_directory)\n",
    "\n",
    "vidcap = cv2.VideoCapture(input_video)\n",
    "\n",
    "if vidcap.isOpened():\n",
    "  frame_count = 0\n",
    "\n",
    "  while True:\n",
    "      ret, frame = vidcap.read()\n",
    "      \n",
    "      if not ret:\n",
    "          break\n",
    "\n",
    "      # Save each frame as an image in the output folder\n",
    "      frame_name = f\"frame_{frame_count:04d}.jpg\"\n",
    "      frame_path = os.path.join(output_directory, frame_name)\n",
    "      cv2.imwrite(frame_path, frame)\n",
    "\n",
    "      frame_count += 1\n",
    "\n",
    "  vidcap.release()\n",
    "\n",
    "  print(f\"Video frames saved in '{output_directory}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Page boundary detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_frame_path = os.path.join(output_directory, \"frame_0000.jpg\")\n",
    "\n",
    "def get_paper_bounds(image_path):\n",
    "        image = cv2.imread(image_path)\n",
    "        cv2.namedWindow('Image')\n",
    "        cv2.imshow('Image', image)\n",
    "\n",
    "        # Select box using a mouse event callback\n",
    "        box = cv2.selectROI('Image', image, fromCenter=False, showCrosshair=True)\n",
    "        cv2.destroyWindow('Image')\n",
    "\n",
    "        # Return bounds: x_min, y_min, x_max, y_max\n",
    "        return int(box[0]), int(box[1]), int(box[0] + box[2]), int(box[1] + box[3])\n",
    "\n",
    "threshold_x_min, threshold_y_min, threshold_x_max, threshold_y_max = get_paper_bounds(first_frame_path)\n",
    "print(threshold_x_max, threshold_x_min, threshold_y_max, threshold_y_min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Annotator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_frames = []\n",
    "\n",
    "selecting  = False\n",
    "highlighted = False\n",
    "colored = False\n",
    "\n",
    "highlight_color = [255, 233, 160]\n",
    "changed_color = [255, 0, 150]\n",
    "\n",
    "max_x, max_y, min_x, min_y = float('-inf'), float('-inf'), float('inf'), float('inf')\n",
    "threshold_x_min, threshold_y_min, threshold_x_max, threshold_y_max = 430, 200, 1200, 800\n",
    "last_gesture = \"\"\n",
    "all_boxes = []\n",
    "\n",
    "for filename in sorted(os.listdir(output_directory)):\n",
    "    print(filename)\n",
    "    frame_path = os.path.join(output_directory, filename)\n",
    "    \n",
    "    if os.path.isfile(frame_path):  # Check if it's a file (not a subdirectory)\n",
    "        # Create image for gesture recognition\n",
    "        image = mp.Image.create_from_file(frame_path)\n",
    "        # Create image that we can draw on\n",
    "        img = cv2.imread(frame_path)\n",
    "        img_height, img_width = img.shape[:2]\n",
    "        \n",
    "        # For each frame, identify the hand gesture in the input image\n",
    "        recognition_result = recognizer.recognize(image)\n",
    "        \n",
    "        if recognition_result.gestures:\n",
    "            # Get the most probable gesture of the hand\n",
    "            # Format: Category(index, score, display_name, category_name)\n",
    "            top_gesture = recognition_result.gestures[0][0] \n",
    "        else:\n",
    "            top_gesture = \"None\"\n",
    "        \n",
    "        # Perform an action for each given gesture\n",
    "        if top_gesture != \"None\":\n",
    "            gesture = top_gesture.category_name\n",
    "            hand_landmarks = recognition_result.hand_landmarks\n",
    "\n",
    "            if gesture == \"Open_Palm\": # Start or stop selecting \n",
    "                if not selecting and last_gesture != \"Closed\":\n",
    "                    # Start selecting and reset variables\n",
    "                    selecting = True\n",
    "\n",
    "                    highlighted = False\n",
    "                    colored = False\n",
    "                    mask = np.zeros((img.shape[0], img.shape[1], 3), dtype=\"uint8\")\n",
    "                    max_x, max_y, min_x, min_y = float('-inf'), float('-inf'), float('inf'), float('inf')\n",
    "                    all_boxes = []\n",
    "\n",
    "                elif selecting and last_gesture == \"Selecting\":\n",
    "                    # Stop selecting and assign the bounding  box\n",
    "                    selecting = False\n",
    "\n",
    "                    if max_x > min_x and max_y > min_y:\n",
    "                        all_boxes.append((min_x, max_x, min_y, max_y))   \n",
    "\n",
    "                last_gesture = \"Closed\"\n",
    "            elif gesture == \"None\" and last_gesture == \"Closed\":\n",
    "                # Sometimes there are gestures during a sequence of open_palm frames that get misidentified as \"None\", so assign them as \"Closed\" to keep the selection going\n",
    "                last_gesture = \"Closed\"\n",
    "\n",
    "            elif selecting: # Speecify the selection box coordinates\n",
    "                # If we are selecting, this should be a Thumbs_Up gesture, but don't check in case it gets misclassified\n",
    "                # Thumb tip is landmark index 4\n",
    "                thumb_landmark = hand_landmarks[0][4] \n",
    "                ind_x = int(thumb_landmark.x * img_width)\n",
    "                ind_y = int(thumb_landmark.y * img_height)\n",
    "\n",
    "                if visualize_tracking:\n",
    "                    cv2.putText(img, f\"Thumb coordinates: ({ind_x}, {ind_y})\",  (50, 150), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "                    cv2.circle(img, (ind_x, ind_y), 5, (0, 0, 255), -1)  # -1 means a filled circle\n",
    "\n",
    "                # If the thumb coordinate is not an outlier, update the max and min values\n",
    "                if (threshold_x_min <= ind_x <= threshold_x_max) and (threshold_y_min <= ind_y <= threshold_y_max):\n",
    "                    max_x = max(max_x, ind_x)\n",
    "                    max_y = max(max_y, ind_y)\n",
    "                    min_x = min(min_x, ind_x)\n",
    "                    min_y = min(min_y, ind_y)\n",
    "\n",
    "                last_gesture = \"Selecting\"\n",
    "\n",
    "            elif not selecting: # Perform an annotation action\n",
    "                if gesture == \"Victory\": # Highlight\n",
    "                    if not highlighted: \n",
    "                        # Get the most recent box\n",
    "                        min_x, max_x, min_y, max_y = all_boxes[len(all_boxes)-1]\n",
    "\n",
    "                        # Update mask: iterate through box and set pixels above a certain intensity (white) to the highlight color\n",
    "                        for i_i in range(min_y, max_y):\n",
    "                            for j in range(min_x, max_x):\n",
    "                                if sum(img[i_i][j]) > 660: # 220*3\n",
    "                                    mask[i_i][j] = highlight_color\n",
    "                        \n",
    "                        highlighted = True \n",
    "                        \n",
    "                elif gesture == \"ILoveYou\": # Change text color\n",
    "                    if not colored: \n",
    "                        # Get the most recent box\n",
    "                        min_x, max_x, min_y, max_y = all_boxes[len(all_boxes)-1]\n",
    "                        \n",
    "                        # Update mask: iterate through box and set pixels below a certain intensity (black) to the new text color\n",
    "                        for i_i in range(min_y, max_y):\n",
    "                            for j in range(min_x, max_x):\n",
    "                                if sum(img[i_i][j]) < 300:\n",
    "                                    mask[i_i][j] = changed_color\n",
    "                        \n",
    "                        colored = True\n",
    "                \n",
    "                elif gesture == \"Closed_Fist\": # Erase\n",
    "                    if highlighted or colored:\n",
    "                        # Reset booleans and mask\n",
    "                        highlighted = False\n",
    "                        colored = False\n",
    "                        mask = np.zeros((img.shape[0], img.shape[1], 3), dtype=\"uint8\")\n",
    " \n",
    "                last_gesture = \"Nonselecting\"\n",
    "            \n",
    "            # Apply mask to frame if we have highlighted or changed color\n",
    "            if highlighted or colored:\n",
    "                updated_mask_sum = np.sum(mask, axis = -1)\n",
    "                for i_i in range(updated_mask_sum.shape[0]):\n",
    "                    for j in range(updated_mask_sum.shape[1]):\n",
    "                        if updated_mask_sum[i_i][j] > 0:\n",
    "                            if list(img[i_i][j]) != list(mask[i_i][j]):\n",
    "                                img[i_i][j] = mask[i_i][j]\n",
    "            \n",
    "            if visualize_tracking:\n",
    "                # Add text to the image\n",
    "                cv2.putText(img, f\"Gesture: {gesture}\",  (50, 100), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "\n",
    "                # Add selection box to the image\n",
    "                for min_x, max_x, min_y, max_y in all_boxes:\n",
    "                    cv2.rectangle(img, (int(min_x), int(min_y)), (int(max_x), int(max_y)), (0, 255, 0), 2)\n",
    "\n",
    "                # # Display the image with the added text\n",
    "                # cv2.imshow('Image with Text', img)\n",
    "                # cv2.waitKey(5)\n",
    "                # cv2.destroyAllWindows()\n",
    "\n",
    "            # Add frame to video_frames array\n",
    "            video_frames.append(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile video from saved output frames\n",
    "create_video(video_frames, output_video)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
